{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b23894c-d552-4f3a-a65b-9f12b9d6be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "import random\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d140e389-cf16-40eb-b078-6a1d0da9e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "config = {\n",
    "    'epochs': 150,\n",
    "    'batch_size' : 8,\n",
    "    'context' : 48,\n",
    "    'learning_rate' : 0.001,\n",
    "    'architecture' : 'very-low-cutoff'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a679e7-dd33-4d0c-86ed-856b2370467e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91978124-13ba-4ad8-bc3e-ded5bcdccf54",
   "metadata": {},
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11f23228-685e-4720-8f0b-1f0be05773b8",
   "metadata": {},
   "source": [
    "am_path = \"./penstate_data/AMs_final.csv\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "58e4107c-b4d1-4952-ab76-3b8ea7c50334",
   "metadata": {},
   "source": [
    "am_data= pd.read_csv(am_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "680af085-b511-4732-ac12-3f9ecf0c009a",
   "metadata": {},
   "source": [
    "am_data.ID.values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e25138b6-0e18-491c-a426-2c8ddc5bbd0c",
   "metadata": {},
   "source": [
    "root_path = \"./penstate_data/download/Full_voice_files\"\n",
    "gender = \"ADAPT_Male_voice_recordings\" # ADAPT_Male_voice_recordings ADAPT_Female_voice_recordings\n",
    "target_voice_path = \"/\".join([root_path, gender])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27747e33-25e0-4674-8190-e620b98d9849",
   "metadata": {
    "tags": []
   },
   "source": [
    "for file in os.listdir(target_voice_path):\n",
    "    person_id = int(file[1:7])\n",
    "    if person_id in am_data.ID.values:\n",
    "        copyfile(target_voice_path + \"/\" + file, \"./penstate_data/download/Full_voice_files/Male_processed/\" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1a204-232f-44e6-96c8-52dcb7a44425",
   "metadata": {},
   "source": [
    "# padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df193a7-1f65-4ab9-8af6-39681390b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.zeros((2,64,5002))\n",
    "MAX_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db129538-ce65-41a2-93a1-ee68b657f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test = np.pad(temp, ((0, 0),(0,0), (0, MAX_test - temp.shape[2])), 'symmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de029d5-27fd-429d-84f1-a9ea37623e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 64, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_test[:,:,:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfb3951-5a9a-4cf3-8f8b-173a483d1044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113bf0a6-3b35-4b0e-97e3-3a2e992101a2",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a61495c-d324-4401-b3ac-b8100dbf5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_path, am_path, gender = \"Female_processed\", am_idx = 1, MAX_LEN = 128, partition = \"train\"):\n",
    "        \"\"\"\n",
    "        :param data_path: the root path of phonemes\n",
    "        :param am_path: the path of am (.csv)\n",
    "        :param gender: female or male\n",
    "        :param phoneme_idx: the phoneme index\n",
    "        :param am_idx: the index of target AM, should be int within [1, 96]\n",
    "        :param MAX_LEN: max length of voice seq, if less, pad, if more, slice\n",
    "        :param partition: train / val1 / val2 / test\n",
    "        \"\"\"\n",
    "\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        # get phoneme list\n",
    "        self.target_voice_path = \"/\".join([data_path, gender])\n",
    "        voice_list = sorted(os.listdir(self.target_voice_path))\n",
    "        random.shuffle(voice_list)\n",
    "        length = len(voice_list)\n",
    "        if partition == \"train\":\n",
    "            self.voice_list = voice_list[:int(0.7 * length)]\n",
    "        elif partition == \"val1\":\n",
    "            self.voice_list = voice_list[int(0.7 * length):int(0.8 * length)]\n",
    "        elif partition == \"val2\":\n",
    "            self.voice_list = voice_list[int(0.8 * length):int(0.9 * length)]\n",
    "        elif partition == \"test\":\n",
    "            self.voice_list = voice_list[int(0.9 * length):]\n",
    "            \n",
    "        # if partition == \"train\":\n",
    "        #     self.phoneme_list = phoneme_list[:int(0.7 * length)]\n",
    "        # elif partition == \"val1\":\n",
    "        #     self.phoneme_list = phoneme_list[int(0.7 * length):]\n",
    "\n",
    "\n",
    "        self.length = len(self.voice_list)\n",
    "\n",
    "        # get_am data\n",
    "        am_data = pd.read_csv(am_path)\n",
    "        self.am_data = am_data[[\"ID\", str(am_idx)]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def spectro_gram(self, sig, rate_of_sample=44100, n_mels=64, n_fft=512, hop_len=None):\n",
    "        # top_db = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(\n",
    "                sample_rate=rate_of_sample, n_fft=n_fft,\n",
    "                win_length=400, hop_length=160, n_mels=n_mels)(sig)\n",
    "        \n",
    "        # Convert to decibels\n",
    "        # spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        item_filename = self.voice_list[ind]\n",
    "        item_full_path = \"/\".join([self.target_voice_path, item_filename])\n",
    "        \n",
    "        data_waveform, rate_of_sample = torchaudio.load(item_full_path)\n",
    "        # voice = np.load(item_full_path)\n",
    "\n",
    "        person_id = int(item_filename[1:7])\n",
    "        try:\n",
    "            target_am = self.am_data[self.am_data[\"ID\"] == person_id].values[0][-1]\n",
    "        except:\n",
    "            print(\"person id =\", person_id)\n",
    "            target_am = 0.\n",
    "\n",
    "        # padding\n",
    "        data_waveform = torch.tensor(data_waveform, dtype=torch.float) #.reshape(1, -1)\n",
    "        # apply mel transform\n",
    "        data_waveform = self.spectro_gram(data_waveform, rate_of_sample)\n",
    "\n",
    "        std, mean = torch.std_mean(data_waveform, unbiased=False, dim=0)\n",
    "        data_waveform = (data_waveform - mean) / (std + 1e-6)\n",
    "        # print(data_waveform.shape)\n",
    "        if data_waveform.shape[2] < MAX_LEN:\n",
    "            # data_waveform = np.pad(data_waveform, ((0, 0),(0,0), (0, MAX_LEN - data_waveform.shape[2])), 'symmetric'), 'constant', constant_values=(0, 0)\n",
    "            data_waveform = np.pad(data_waveform, ((0, 0),(0,0), (0, MAX_LEN - data_waveform.shape[2])), 'constant', constant_values=(0, 0))\n",
    "            \n",
    "            data_waveform = torch.from_numpy(data_waveform)\n",
    "        else:\n",
    "            temp_start = random.randint(0, data_waveform.shape[2] - MAX_LEN)\n",
    "            data_waveform = data_waveform[:,:,temp_start:temp_start + MAX_LEN]\n",
    "        # print(data_waveform.shape)\n",
    "        # phoneme = torch.from_numpy(phoneme)\n",
    "        ##################################################################\n",
    "        # data_waveform.unsqueeze_(0)\n",
    "        ##################################################################\n",
    "        target_am = torch.tensor(target_am).to(torch.float32)\n",
    "        \n",
    "        return data_waveform, target_am\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870d26e7-ad98-479d-8fcf-f21613bfae08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "#     def __init__(self, data_path, am_path, gender = \"female\", phoneme_idx = 4, am_idx = 1, MAX_LEN = 44100 * 2, partition = \"train\"):\n",
    "#         \"\"\"\n",
    "#         :param data_path: the root path of phonemes\n",
    "#         :param am_path: the path of am (.csv)\n",
    "#         :param gender: female or male\n",
    "#         :param phoneme_idx: the phoneme index\n",
    "#         :param am_idx: the index of target AM, should be int within [1, 96]\n",
    "#         :param MAX_LEN: max length of voice seq, if less, pad, if more, slice\n",
    "#         :param partition: train / val1 / val2 / test\n",
    "#         \"\"\"\n",
    "\n",
    "#         self.MAX_LEN = MAX_LEN\n",
    "#         # get phoneme list\n",
    "#         self.target_phoneme_path = \"/\".join([data_path, gender, str(int(phoneme_idx))])\n",
    "#         phoneme_list = sorted(os.listdir(self.target_phoneme_path))\n",
    "#         length = len(phoneme_list)\n",
    "#         if partition == \"train\":\n",
    "#             self.phoneme_list = phoneme_list[:int(0.7 * length)]\n",
    "#         elif partition == \"val1\":\n",
    "#             self.phoneme_list = phoneme_list[int(0.7 * length):int(0.8 * length)]\n",
    "#         elif partition == \"val2\":\n",
    "#             self.phoneme_list = phoneme_list[int(0.8 * length):int(0.9 * length)]\n",
    "#         elif partition == \"test\":\n",
    "#             self.phoneme_list = phoneme_list[int(0.9 * length):]\n",
    "\n",
    "#         self.length = len(self.phoneme_list)\n",
    "\n",
    "#         # get_am data\n",
    "#         am_data = pd.read_csv(am_path)\n",
    "#         self.am_data = am_data[[\"ID\", str(am_idx)]]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "\n",
    "#     def spectro_gram(self, sig, n_mels=64, n_fft=1024, hop_len=None):\n",
    "#         top_db = 80\n",
    "\n",
    "#         # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "#         spec = transforms.MelSpectrogram(44100, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "#         # Convert to decibels\n",
    "#         spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "#         return spec\n",
    "\n",
    "#     def padding(self, phoneme):\n",
    "#         if len(phoneme) < self.MAX_LEN:\n",
    "#             pad_begin_len = random.randint(0, self.MAX_LEN - len(phoneme))\n",
    "#             pad_end_len = self.MAX_LEN - len(phoneme) - pad_begin_len\n",
    "\n",
    "#             # Pad with 0s\n",
    "#             pad_begin = np.zeros(pad_begin_len)\n",
    "#             pad_end = np.zeros(pad_end_len)\n",
    "\n",
    "#             phoneme = np.concatenate((pad_begin, phoneme, pad_end), 0)\n",
    "#         else:\n",
    "#             phoneme = phoneme[:self.MAX_LEN]\n",
    "#         return phoneme\n",
    "\n",
    "#     def __getitem__(self, ind):\n",
    "#         item_filename = self.phoneme_list[ind]\n",
    "#         item_full_path = \"/\".join([self.target_phoneme_path, item_filename])\n",
    "#         phoneme = np.load(item_full_path)\n",
    "\n",
    "#         person_id = int(item_filename.split(\"_\")[0][1:7])\n",
    "#         try:\n",
    "#             target_am = self.am_data[self.am_data[\"ID\"] == person_id].values[0][-1]\n",
    "#         except:\n",
    "#             print(\"person id =\", person_id)\n",
    "#             target_am = 0.\n",
    "\n",
    "#         # padding\n",
    "#         phoneme = self.padding(phoneme)\n",
    "#         phoneme = torch.tensor(phoneme, dtype=torch.float) #.reshape(1, -1)\n",
    "#         # apply mel transform\n",
    "#         phoneme = self.spectro_gram(phoneme)\n",
    "        \n",
    "#         ################################### Normalization ######################################\n",
    "#         std, mean = torch.std_mean(phoneme, unbiased=False, dim=0)\n",
    "#         phoneme = (phoneme - mean) / (std + 1e-6)\n",
    "#         # print(phoneme)\n",
    "#         # ####################### convert phoneme from float32 to float64 ##################\n",
    "#         # phoneme = phoneme.to(torch.float64)\n",
    "#         # ##################################################################################\n",
    "\n",
    "#         target_am = torch.tensor(target_am)\n",
    "        \n",
    "        \n",
    "#         ####################################################################################\n",
    "#         target_am = target_am.to(torch.float32)\n",
    "#         # print(target_am)\n",
    "#         ####################################################################################\n",
    "        \n",
    "#         # jia yi ge gui yi hua (phoneme)\n",
    "        \n",
    "#         return phoneme, target_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad957703-795b-42ee-b460-4e2bb8c7d9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# default_root_path = \"./penstate_data/extract_phoneme\"\n",
    "default_root_path = \"./penstate_data/download/Full_voice_files\"\n",
    "\n",
    "# am_path = \"./penstate_data/AMs_unnormalized.csv\"\n",
    "am_path = \"./penstate_data/AMs_final.csv\"\n",
    "\n",
    "############## Female ##################\n",
    "gender = \"Female_processed\" # Male_processed\n",
    "am_idx = 89\n",
    "\n",
    "# gender = \"female\"\n",
    "# phoneme_idx = 10\n",
    "# am_idx = 13\n",
    "\n",
    "# gender = \"female\"\n",
    "# phoneme_idx = 10\n",
    "# am_idx = 42\n",
    "\n",
    "# gender = \"female\"\n",
    "# phoneme_idx = 10\n",
    "# am_idx = 7\n",
    "\n",
    "############## Male ##################\n",
    "# gender = \"male\"\n",
    "# phoneme_idx = 10\n",
    "# am_idx = 89\n",
    "\n",
    "# gender = \"male\"\n",
    "# phoneme_idx = 10\n",
    "# am_idx = 51\n",
    "\n",
    "# gender = \"male\"\n",
    "# phoneme_idx = 10\n",
    "# am_idx = 4\n",
    "\n",
    "# gender = \"male\"\n",
    "# phoneme_idx = 10\n",
    "# am_idx = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7abfa6fc-9b28-47d5-bf96-376c4e1ba1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  8\n",
      "Train dataset samples = 482, batches = 61\n",
      "Validation dataset samples = 70, batches = 9\n",
      "Test dataset samples = 70, batches = 9\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 4096 # TODO: may be too small\n",
    "batch_size = config['batch_size']\n",
    "# batch_size = 4\n",
    "train_data = AudioDataset(data_path=default_root_path,\n",
    "                            am_path = am_path,\n",
    "                            gender = gender, am_idx = am_idx, MAX_LEN = MAX_LEN, partition=\"train\")\n",
    "\n",
    "######################################################################################################################################\n",
    "val_data = AudioDataset(data_path=default_root_path,\n",
    "                            am_path = am_path,\n",
    "                            gender = gender, am_idx = am_idx, MAX_LEN = MAX_LEN, partition=\"val1\")\n",
    "test_data = AudioDataset(data_path=default_root_path,\n",
    "                            am_path = am_path,\n",
    "                            gender = gender, am_idx = am_idx, MAX_LEN = MAX_LEN, partition=\"val1\")\n",
    "######################################################################################################################################\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, num_workers=0,\n",
    "                                               batch_size=batch_size, shuffle=True)\n",
    "\n",
    "######################################################################################################################################\n",
    "val_loader = torch.utils.data.DataLoader(val_data, num_workers=0,\n",
    "                                               batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, num_workers=0,\n",
    "                                               batch_size=batch_size)\n",
    "######################################################################################################################################\n",
    "\n",
    "print(\"Batch size: \", config['batch_size'])\n",
    "\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d141bba-f6ad-4725-9949-718537cd97a2",
   "metadata": {},
   "source": [
    "all_am = None\n",
    "for i, data in enumerate(train_loader):\n",
    "    phoneme, target_am = data\n",
    "    # sns.heatmap(phoneme[0], cmap=\"rainbow\")\n",
    "    # plt.show()\n",
    "    if all_am is None:\n",
    "        all_am = target_am\n",
    "    else:\n",
    "        all_am = torch.cat([all_am, target_am])\n",
    "    # print(phoneme.shape, target_am.shape)\n",
    "    # break\n",
    "with open(gender + \"_am.txt\", \"a+\") as f:\n",
    "    f.write(f'{phoneme_idx},{am_idx},{all_am.mean().item()}\\n')\n",
    "print(f'{phoneme_idx},{am_idx},{all_am.mean().item()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76c74d99-578e-4710-b5c7-cd3783489de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  8\n",
      "Train dataset samples = 482, batches = 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1097604/2427131139.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_waveform = torch.tensor(data_waveform, dtype=torch.float) #.reshape(1, -1)\n",
      "/home/oscar/anaconda3/envs/torch_project/lib/python3.9/site-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voice shape = torch.Size([8, 2, 64, 4096]) target_am shape = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "for i, data in enumerate(train_loader):\n",
    "    voice, target_am = data\n",
    "    print(\"voice shape =\",voice.shape, \"target_am shape =\",target_am.shape)\n",
    "    break\n",
    "# for i, data in enumerate(train_loader):\n",
    "#     phoneme, target_am = data\n",
    "#     print(phoneme.shape, target_am.shape)\n",
    "#     ##########################################\n",
    "#     # print(phoneme.dtype, target_am.dtype)\n",
    "#     ##########################################\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db9324-d5e9-4cc3-880d-fe3ed5d1b216",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eaeef63d-9e83-41cc-84cb-0eba41760795",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Model 1: CNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f9842b8-efda-47d9-a471-ec6fd119027f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# class CNNNetwork(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1=nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3,stride=1,padding=2),\n",
    "#             nn.BatchNorm2d(num_features=16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "#         self.conv2=nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=2),\n",
    "#             nn.BatchNorm2d(num_features=32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "#         self.conv3=nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=2),\n",
    "#             nn.BatchNorm2d(num_features=64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "#         self.conv4=nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=2),\n",
    "#             nn.BatchNorm2d(num_features=128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "#         self.flatten=nn.Flatten()\n",
    "#         self.linear1=nn.Linear(in_features=128*15,out_features=512)\n",
    "#         self.linear2=nn.Linear(in_features=512,out_features=128)\n",
    "#         self.linear3=nn.Linear(in_features=128,out_features=1)\n",
    "#         # self.linear4=nn.Linear(in_features=1024,out_features=256)\n",
    "#         # self.linear5=nn.Linear(in_features=256,out_features=128)\n",
    "#         # self.linear6=nn.Linear(in_features=128,out_features=1)\n",
    "#         # self.output=nn.Sigmoid()\n",
    "#         self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "#         self.output = nn.Tanh()\n",
    "    \n",
    "#     def forward(self,input_data):\n",
    "#         # add one dimension\n",
    "#         # input_data.unsqueeze_(1)\n",
    "#         x=self.conv1(input_data)\n",
    "#         x=self.conv2(x)\n",
    "#         x=self.conv3(x)\n",
    "#         x=self.conv4(x)\n",
    "        \n",
    "#         # x = self.pooling(x)\n",
    "#         # print(\"After conv: \", x.shape)\n",
    "#         x=self.flatten(x)\n",
    "#         # print(\"After flatten: \", x.shape)\n",
    "#         x=self.linear1(x)\n",
    "#         # print(\"After linear: \",x.shape)\n",
    "#         x=self.linear2(x)\n",
    "#         # x=self.linear3(x)\n",
    "#         # x=self.linear4(x)\n",
    "#         # x=self.linear5(x)\n",
    "        \n",
    "#         logits=self.linear3(x)\n",
    "#         output=self.output(logits)\n",
    "#         # print(output)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01da6240-e6aa-4d15-89b0-9bd20f1a8e6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# model = CNNNetwork().to(device)\n",
    "# phoneme, AM = next(iter(train_loader))\n",
    "# # # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66850381-d103-4d15-af6f-fd6ffa15f202",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model 2: Resnet50"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c474433b-e056-46cf-90dc-53cfb0390fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# model = models.resnet50(weights=None).to(device) # may be too weak\n",
    "model = models.resnet152(weights=None).to(device) # may be too weak\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 1).to(device)\n",
    "# print(model.conv1)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4188befc-952c-4043-9f78-a8089619cef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = model.to(device)\n",
    "phoneme, AM = next(iter(train_loader))\n",
    "# # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f71bdff-0166-458e-8a3c-ec1e9cb55490",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model 3: DenseNet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73567992-a037-4fa1-8775-35713728d1c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = models.densenet121(weights=None).to(device) # may be too weak\n",
    "\n",
    "# num_features = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_features, 1).to(device)\n",
    "# print(model)\n",
    "model.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.classifier = nn.Linear(in_features=1024, out_features=1, bias=True)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f42bab4-e0bf-45af-bada-91f5b88bd061",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = model.to(device)\n",
    "phoneme, AM = next(iter(train_loader))\n",
    "# # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5468244a-796c-4801-92ca-f8449391a81b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model 4: EfficientNetV2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a736d83-f333-470c-a3a2-bdf832bc0c7a",
   "metadata": {},
   "source": [
    "MAE: 0.69??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09b63a5c-5655-40ed-b2ee-67ade5e2e498",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = models.efficientnet_v2_s(weights=None).to(device) # may be too weak\n",
    "# print(model)\n",
    "model.features[0][0] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "model.classifier = nn.Linear(in_features=1280, out_features=1, bias=True)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f79e62c3-692a-42a3-80b6-6dd716095e4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = model.to(device)\n",
    "phoneme, AM = next(iter(train_loader))\n",
    "# # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b3b1605-c19f-4d44-a731-ac8680c40b87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model 5: MobileNetV3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c606133-c5bb-4a6a-8c65-a968b1ab2ea2",
   "metadata": {},
   "source": [
    "##### MAE: 0.68??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec4d7ae2-c804-467c-89f5-325cf820b47e",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = models.mobilenet_v3_large(weights=None).to(device)\n",
    "# print(model)\n",
    "model.features[0][0] = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "model.classifier[3] = nn.Linear(in_features=1280, out_features=1, bias=True)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5401a006-a2b5-4eab-b17d-653ba234377b",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = model.to(device)\n",
    "phoneme, AM = next(iter(train_loader))\n",
    "# # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1616fa20-d08e-488d-96a4-79391030cc19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model 6: ShuffleNetV2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c15b6f1-f9b1-4632-86f8-218df6a4da3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = models.shufflenet_v2_x1_0(weights=None).to(device)\n",
    "# print(model)\n",
    "model.conv1[0] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "model.fc = nn.Linear(in_features=1024, out_features=1, bias=True)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70832dae-ebe2-44be-b63f-ec74b54381c5",
   "metadata": {},
   "source": [
    "model = model.to(device)\n",
    "phoneme, AM = next(iter(train_loader))\n",
    "# # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3da36707-c2f5-4ad5-8181-32e648ff15d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 7: SqueezeNet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3ed1ab2-498e-4b32-bbf6-fa5a7290787c",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = models.squeezenet1_1(weights=None).to(device)\n",
    "# print(model)\n",
    "model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "model.classifier[1] = nn.Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "caa74b5f-7077-4003-a37f-780e22bbbafb",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = model.to(device)\n",
    "phoneme, AM = next(iter(train_loader))\n",
    "# # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ec3e0-2820-47fc-85d0-2542b1b372e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 8: MnasNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd549074-a6a7-450d-a7be-97dd639f07b9",
   "metadata": {},
   "source": [
    "##### MAE=0.68 ok???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdff1e5c-80fc-4826-af6f-695f4b056073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.mnasnet1_0(weights=None).to(device)\n",
    "# # print(model)\n",
    "# model.layers[0] = nn.Conv2d(2, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "# model.classifier[1] = nn.Linear(in_features=1280, out_features=1, bias=True)\n",
    "# # print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd8e0ff-fc4d-4a69-a75b-8a1a2be686ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1097604/2427131139.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_waveform = torch.tensor(data_waveform, dtype=torch.float) #.reshape(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 1280, 2, 128]        --\n",
      "|    └─Conv2d: 2-1                       [-1, 32, 32, 2048]        576\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 32, 32, 2048]        64\n",
      "|    └─ReLU: 2-3                         [-1, 32, 32, 2048]        --\n",
      "|    └─Conv2d: 2-4                       [-1, 32, 32, 2048]        288\n",
      "|    └─BatchNorm2d: 2-5                  [-1, 32, 32, 2048]        64\n",
      "|    └─ReLU: 2-6                         [-1, 32, 32, 2048]        --\n",
      "|    └─Conv2d: 2-7                       [-1, 16, 32, 2048]        512\n",
      "|    └─BatchNorm2d: 2-8                  [-1, 16, 32, 2048]        32\n",
      "|    └─Sequential: 2-9                   [-1, 24, 16, 1024]        --\n",
      "|    |    └─_InvertedResidual: 3-1       [-1, 24, 16, 1024]        2,592\n",
      "|    |    └─_InvertedResidual: 3-2       [-1, 24, 16, 1024]        4,440\n",
      "|    |    └─_InvertedResidual: 3-3       [-1, 24, 16, 1024]        4,440\n",
      "|    └─Sequential: 2-10                  [-1, 40, 8, 512]          --\n",
      "|    |    └─_InvertedResidual: 3-4       [-1, 40, 8, 512]          6,776\n",
      "|    |    └─_InvertedResidual: 3-5       [-1, 40, 8, 512]          13,160\n",
      "|    |    └─_InvertedResidual: 3-6       [-1, 40, 8, 512]          13,160\n",
      "|    └─Sequential: 2-11                  [-1, 80, 4, 256]          --\n",
      "|    |    └─_InvertedResidual: 3-7       [-1, 80, 4, 256]          35,920\n",
      "|    |    └─_InvertedResidual: 3-8       [-1, 80, 4, 256]          90,880\n",
      "|    |    └─_InvertedResidual: 3-9       [-1, 80, 4, 256]          90,880\n",
      "|    └─Sequential: 2-12                  [-1, 96, 4, 256]          --\n",
      "|    |    └─_InvertedResidual: 3-10      [-1, 96, 4, 256]          90,912\n",
      "|    |    └─_InvertedResidual: 3-11      [-1, 96, 4, 256]          118,272\n",
      "|    └─Sequential: 2-13                  [-1, 192, 2, 128]         --\n",
      "|    |    └─_InvertedResidual: 3-12      [-1, 192, 2, 128]         182,976\n",
      "|    |    └─_InvertedResidual: 3-13      [-1, 192, 2, 128]         476,160\n",
      "|    |    └─_InvertedResidual: 3-14      [-1, 192, 2, 128]         476,160\n",
      "|    |    └─_InvertedResidual: 3-15      [-1, 192, 2, 128]         476,160\n",
      "|    └─Sequential: 2-14                  [-1, 320, 2, 128]         --\n",
      "|    |    └─_InvertedResidual: 3-16      [-1, 320, 2, 128]         605,440\n",
      "|    └─Conv2d: 2-15                      [-1, 1280, 2, 128]        409,600\n",
      "|    └─BatchNorm2d: 2-16                 [-1, 1280, 2, 128]        2,560\n",
      "|    └─ReLU: 2-17                        [-1, 1280, 2, 128]        --\n",
      "├─Sequential: 1-2                        [-1, 1]                   --\n",
      "|    └─Dropout: 2-18                     [-1, 1280]                --\n",
      "|    └─Linear: 2-19                      [-1, 1]                   1,281\n",
      "==========================================================================================\n",
      "Total params: 3,103,305\n",
      "Trainable params: 3,103,305\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 206.13\n",
      "==========================================================================================\n",
      "Input size (MB): 16.00\n",
      "Forward/backward pass size (MB): 85.00\n",
      "Params size (MB): 11.84\n",
      "Estimated Total Size (MB): 112.84\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 1280, 2, 128]        --\n",
       "|    └─Conv2d: 2-1                       [-1, 32, 32, 2048]        576\n",
       "|    └─BatchNorm2d: 2-2                  [-1, 32, 32, 2048]        64\n",
       "|    └─ReLU: 2-3                         [-1, 32, 32, 2048]        --\n",
       "|    └─Conv2d: 2-4                       [-1, 32, 32, 2048]        288\n",
       "|    └─BatchNorm2d: 2-5                  [-1, 32, 32, 2048]        64\n",
       "|    └─ReLU: 2-6                         [-1, 32, 32, 2048]        --\n",
       "|    └─Conv2d: 2-7                       [-1, 16, 32, 2048]        512\n",
       "|    └─BatchNorm2d: 2-8                  [-1, 16, 32, 2048]        32\n",
       "|    └─Sequential: 2-9                   [-1, 24, 16, 1024]        --\n",
       "|    |    └─_InvertedResidual: 3-1       [-1, 24, 16, 1024]        2,592\n",
       "|    |    └─_InvertedResidual: 3-2       [-1, 24, 16, 1024]        4,440\n",
       "|    |    └─_InvertedResidual: 3-3       [-1, 24, 16, 1024]        4,440\n",
       "|    └─Sequential: 2-10                  [-1, 40, 8, 512]          --\n",
       "|    |    └─_InvertedResidual: 3-4       [-1, 40, 8, 512]          6,776\n",
       "|    |    └─_InvertedResidual: 3-5       [-1, 40, 8, 512]          13,160\n",
       "|    |    └─_InvertedResidual: 3-6       [-1, 40, 8, 512]          13,160\n",
       "|    └─Sequential: 2-11                  [-1, 80, 4, 256]          --\n",
       "|    |    └─_InvertedResidual: 3-7       [-1, 80, 4, 256]          35,920\n",
       "|    |    └─_InvertedResidual: 3-8       [-1, 80, 4, 256]          90,880\n",
       "|    |    └─_InvertedResidual: 3-9       [-1, 80, 4, 256]          90,880\n",
       "|    └─Sequential: 2-12                  [-1, 96, 4, 256]          --\n",
       "|    |    └─_InvertedResidual: 3-10      [-1, 96, 4, 256]          90,912\n",
       "|    |    └─_InvertedResidual: 3-11      [-1, 96, 4, 256]          118,272\n",
       "|    └─Sequential: 2-13                  [-1, 192, 2, 128]         --\n",
       "|    |    └─_InvertedResidual: 3-12      [-1, 192, 2, 128]         182,976\n",
       "|    |    └─_InvertedResidual: 3-13      [-1, 192, 2, 128]         476,160\n",
       "|    |    └─_InvertedResidual: 3-14      [-1, 192, 2, 128]         476,160\n",
       "|    |    └─_InvertedResidual: 3-15      [-1, 192, 2, 128]         476,160\n",
       "|    └─Sequential: 2-14                  [-1, 320, 2, 128]         --\n",
       "|    |    └─_InvertedResidual: 3-16      [-1, 320, 2, 128]         605,440\n",
       "|    └─Conv2d: 2-15                      [-1, 1280, 2, 128]        409,600\n",
       "|    └─BatchNorm2d: 2-16                 [-1, 1280, 2, 128]        2,560\n",
       "|    └─ReLU: 2-17                        [-1, 1280, 2, 128]        --\n",
       "├─Sequential: 1-2                        [-1, 1]                   --\n",
       "|    └─Dropout: 2-18                     [-1, 1280]                --\n",
       "|    └─Linear: 2-19                      [-1, 1]                   1,281\n",
       "==========================================================================================\n",
       "Total params: 3,103,305\n",
       "Trainable params: 3,103,305\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 206.13\n",
       "==========================================================================================\n",
       "Input size (MB): 16.00\n",
       "Forward/backward pass size (MB): 85.00\n",
       "Params size (MB): 11.84\n",
       "Estimated Total Size (MB): 112.84\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model.to(device)\n",
    "# phoneme, AM = next(iter(train_loader))\n",
    "# # # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4455c-36c8-4867-930d-8cf1957d7557",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 9: Wide ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46827b9f-e1cd-431f-9863-c8c815921792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = models.mnasnet1_0(weights=None).to(device)\n",
    "# # print(model)\n",
    "# model.layers[0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "# model.classifier[1] = nn.Linear(in_features=1280, out_features=1, bias=True)\n",
    "# # print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58bcf8c1-e9a7-4619-b299-258cd29d394d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "# phoneme, AM = next(iter(train_loader))\n",
    "# # # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# # summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a5bd7-2d59-4282-8ebc-2683628cdce6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10dfea54-c692-473e-83b4-595b33490feb",
   "metadata": {},
   "source": [
    "## Model 10: VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d31e8eb3-5094-4c70-b2c1-21972d9172a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=None).to(device)\n",
    "# print(model)\n",
    "model.features[0] = nn.Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "model.classifier.append(nn.ReLU(inplace=True))\n",
    "model.classifier.append(nn.Dropout(p=0.5, inplace=False))\n",
    "model.classifier.append(nn.Linear(in_features=1000, out_features=1, bias=True))\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2e31f-9cdc-4c41-9c4e-f5b9e6ff0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "# phoneme, AM = next(iter(train_loader))\n",
    "# # # summary(model,(64, 259)) # After conv: torch.Size([2, 128, 5, 18])\n",
    "# summary(model, phoneme.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff3c7e-600a-4f48-a73e-3477e6d08404",
   "metadata": {},
   "source": [
    "# Train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d6978e0-43a9-4a1b-9f05-7fca1d25be4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563a8ca6-0732-4409-a74c-1e2ef43191b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() #Defining Loss function \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) #Defining Optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.0001, last_epoch=-1)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[35,40,45,50,60,65,70,90,110,150,170,180], gamma=0.5) # add learning rate scheduler\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * config['epochs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75674d65-bd4f-412b-8aa0-2d951cae0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataloader):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0 #Monitoring Loss\n",
    "    \n",
    "    #########################################################\n",
    "    # AM_true_list = []\n",
    "    # AM_pred_list = []\n",
    "    #########################################################\n",
    "    \n",
    "    for iter, (phoneme, AM) in enumerate(dataloader):\n",
    "        scheduler.step()\n",
    "        ### Move Data to Device (Ideally GPU)\n",
    "        phoneme = phoneme.to(device)\n",
    "        AM = AM.to(device)\n",
    "\n",
    "        ### Forward Propagation\n",
    "        preds_AM = model(phoneme)\n",
    "\n",
    "        ### Loss Calculation\n",
    "        # print(AM.shape)\n",
    "        preds_AM = torch.squeeze(preds_AM)\n",
    "        # print(preds_AM)\n",
    "        # print(preds_AM.shape)model = models.shufflenet_v2_x1_0(weights=None).to(device)\n",
    "        loss = criterion(preds_AM, AM)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        #########################################################\n",
    "        ### Store Pred and True Labels\n",
    "        # AM_pred_list.extend(preds_AM.cpu().tolist())\n",
    "        # AM_true_list.extend(AM.cpu().tolist())\n",
    "        #########################################################\n",
    "\n",
    "        ### Initialize Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ### Backward Propagation\n",
    "        loss.backward()\n",
    "\n",
    "        ### Gradient Descent\n",
    "        optimizer.step()\n",
    "        # if iter % 20 == 0:\n",
    "        #     print(\"iter =\", iter, \"loss =\",loss.item())\n",
    "    train_loss /= len(dataloader)\n",
    "    print(\"Learning rate = \", scheduler.get_last_lr()[0])\n",
    "    print(\"Train loss = \", train_loss)\n",
    "    \n",
    "    #########################################################\n",
    "    # print(AM_pred_list)\n",
    "    # print(AM_true_list)\n",
    "    # print(len(AM_pred_list))\n",
    "    # print(len(AM_true_list))\n",
    "    # accuracy = mean_squared_error(AM_pred_list, AM_true_list)\n",
    "    # print(\"Train MSE accuracy: \", accuracy)\n",
    "    #########################################################\n",
    "    \n",
    "    # scheduler.step() # add schedule learning rate\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a1518e3-0b07-4b4d-a192-54e5bc0a7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "\n",
    "    model.eval() # set model in evaluation mode\n",
    "\n",
    "    AM_true_list = []\n",
    "    AM_pred_list = []\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        phoneme, AM = data\n",
    "        ### Move data to device (ideally GPU)\n",
    "        phoneme, AM = phoneme.to(device), AM.to(device) \n",
    "\n",
    "        with torch.inference_mode(): # makes sure that there are no gradients computed as we are not training the model now\n",
    "            ### Forward Propagation\n",
    "            ### Get Predictions\n",
    "            predicted_AM = model(phoneme)\n",
    "            # print(predicted_AM)\n",
    "        \n",
    "        ### Store Pred and True Labels\n",
    "        AM_pred_list.extend(predicted_AM.cpu().tolist())\n",
    "        AM_true_list.extend(AM.cpu().tolist())\n",
    "        \n",
    "        # Do you think we need loss.backward() and optimizer.step() here?\n",
    "    \n",
    "        del phoneme, AM, predicted_AM\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    ###############################################################################################\n",
    "    # print(AM_pred_list[1000:3100])\n",
    "    # print(AM_true_list)\n",
    "    # print(len(AM_pred_list))\n",
    "    # print(len(AM_true_list))\n",
    "    ###############################################################################################\n",
    "    \n",
    "    # print(\"Number of equals between two list: \", sum(a == b for a,b in zip(AM_pred_list, AM_true_list)))\n",
    "    \n",
    "    ### Calculate Accuracy\n",
    "    MSE = mean_squared_error(AM_pred_list, AM_true_list)\n",
    "    r2_score_acc = r2_score(AM_pred_list, AM_true_list)\n",
    "    MAE = mean_absolute_error(AM_pred_list, AM_true_list)\n",
    "    print(\"Validation r2_score: \", r2_score_acc)\n",
    "    print(\"Validation MAE: \", MAE)\n",
    "    \n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce9f12-c7c9-4328-b4e2-b921edd4747c",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90eda49-e81d-42a2-af9b-f6f18ef88248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1097604/2427131139.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_waveform = torch.tensor(data_waveform, dtype=torch.float) #.reshape(1, -1)\n",
      "/home/oscar/anaconda3/envs/torch_project/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "# Iterate over number of epochs to train and evaluate your model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "best_mse = 1.0 ### Monitor best accuracy in your run\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    train_loss = train(model, optimizer, criterion, train_loader)\n",
    "    MSE = eval(model, val_loader)\n",
    "\n",
    "    print(\"\\tTrain Loss: \", train_loss)\n",
    "    print(\"\\tValidation MSE: \", MSE)\n",
    "\n",
    "    ### Save checkpoint if accuracy is better than your current best\n",
    "    if MSE < best_mse:\n",
    "        best_mse = MSE\n",
    "    ### Save checkpoint with information you want\n",
    "        torch.save({'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': train_loss,\n",
    "              'learning rate': scheduler.get_last_lr()[0],\n",
    "              'mse': MSE}, \n",
    "        './model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c785492a-0e14-4148-8878-20ad6b73e85a",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bcdd9a-731e-45b8-abd8-7d7e43c8acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "  ### What you call for model to perform inference?\n",
    "    model.eval()\n",
    "\n",
    "  ### List to store predicted phonemes of test data\n",
    "    test_predictions = []\n",
    "    ground_truth = []\n",
    "\n",
    "  ### Which mode do you need to avoid gradients?\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for i, data in enumerate(tqdm(test_loader)):\n",
    "\n",
    "            phoneme, groundtruth_AM = data\n",
    "            ### Move data to device (ideally GPU)\n",
    "            phoneme, groundtruth_AM = phoneme.to(device), groundtruth_AM.to(device)         \n",
    "          \n",
    "            predicted_AM = model(phoneme)\n",
    "            predicted_AM.squeeze_()\n",
    "            # print(predicted_AM.shape)\n",
    "            # print(groundtruth_AM.shape)\n",
    "\n",
    "          ### How do you store predicted_phonemes with test_predictions? Hint, look at eval \n",
    "            test_predictions.extend(predicted_AM.cpu().tolist())\n",
    "            ground_truth.extend(groundtruth_AM.cpu())\n",
    "    \n",
    "    # print(len(test_predictions))\n",
    "    return test_predictions, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a867d0-7ad4-4856-8675-980cacf391a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, ground_truth = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505f8f3-6e26-44f5-964c-610aeae42b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create CSV file with predictions\n",
    "if gender == \"female\":\n",
    "    g_flag = \"F\"\n",
    "else:\n",
    "    g_flag = \"M\"\n",
    "    \n",
    "with open(\"./%s_\"%g_flag + \"phoneme%s\"%phoneme_idx +  \"_AM%s.csv\"%am_idx, \"w+\") as f:\n",
    "    f.write(\"person, label, prediction\\n\")\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(\"{},{},{}\\n\".format(i, ground_truth[i], predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77424a6-3f16-45d2-a290-a6e90508035f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fd8331e-ad70-4eb5-a67b-9f11cb329bcc",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_project",
   "language": "python",
   "name": "torch_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
